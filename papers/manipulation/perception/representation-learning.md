## Representation Learning for Manipulation

[2024] [QueST: Self-Supervised Skill Abstractions for Learning Continuous Control](https://arxiv.org/abs/2407.15840)



### Vision Representation

[2022] [R3M: A Universal Visual Representation for Robot Manipulation](https://arxiv.org/abs/2203.12601)

[2023] [Language-Driven Representation Learning for Robotics](https://arxiv.org/abs/2302.12766)

[2024] [Recasting Generic Pretrained Vision Transformers As Object-Centric Scene Encoders For Manipulation Policies](https://arxiv.org/abs/2405.15916)

[2024] [Learning Manipulation by Predicting Interaction](https://arxiv.org/abs/2406.00439)

[2024] [Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control](https://arxiv.org/abs/2406.06072)



### Dense Correspondence Learning

[2018] [Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation](https://arxiv.org/abs/1806.08756)

[2024] [UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence](https://arxiv.org/abs/2405.06903)



### Affordance

[2023] [Affordances from Human Videos as a Versatile Representation for Robotics](https://arxiv.org/abs/2304.08488)

[2023] [Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation](https://arxiv.org/abs/2303.11057)

[2024] [Learning Precise Affordances from Egocentric Videos for Robotic Manipulation](https://arxiv.org/abs/2408.10123)