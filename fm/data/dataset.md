## Datasets

- [Ego4D](https://ego4d-data.org/): A massive-scale, egocentric dataset and benchmark suite collected across 74 worldwide locations and 9 countries, with over **3,670** hours of daily-life activity video.
- [Language-Table](https://interactive-language.github.io/): comprising nearly 600,000 language-labeled trajectories, an order of magnitude larger than prior available datasets.
- [Open X-Embodiment Dataset](https://robotics-transformer-x.github.io/): assembling a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks).
- [RT-1](https://robotics-transformer1.github.io/): collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months.
- [Something-Somthing v2](https://developer.qualcomm.com/software/ai-datasets/something-something): a collection of 220,847 labeled video clips of humans performing pre-defined, basic actions with everyday objects.
- [VIMA-Data](https://huggingface.co/datasets/VIMA/VIMA-Data): This is the official dataset used to train general robot manipulation agents with multimodal prompts.

