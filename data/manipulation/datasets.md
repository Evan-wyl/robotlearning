# Datasets

- [BC-Z](https://sites.google.com/view/bc-z/home): A large-scale VR-teleoperated dataset of demonstrations for 100 manipulation tasks, and train a convolutional neural network to imitate closed-loop actions from RGB pixel observations.
- [Block-Push](https://diffusion-policy.cs.columbia.edu/data/training/):  Pushing two blocks into two squares in any order.
- [DROID](https://droid-dataset.github.io/): A diverse robot manipulation dataset with 76k demonstration trajectories or 350h of interaction data, collected across 564 scenes and 86 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months.
- [Franka Kitchen](https://robotics.farama.org/envs/franka_kitchen/) or [Kitchen](https://diffusion-policy.cs.columbia.edu/data/training/): Multitask environment in which a 9-DoF Franka robot is placed in a kitchen containing several common household items. 
- [Grasp-Anything](https://arxiv.org/abs/2309.09818): Large-scale Grasp Dataset from Foundation Models.
- [MIME](https://sites.google.com/view/mimedataset):  The largest available robotic-demonstration dataset (MIME) that contains 8260 human-robot demonstrations over 20 different robotic tasks .
- [MimicGen](https://mimicgen.github.io/): A system for automatically synthesizing large-scale, rich datasets from only a small number of human demonstrations by adapting them to new contexts.
- [Push-T](https://diffusion-policy.cs.columbia.edu/data/training/): Pushing a T shaped block (gray) to a fixed target (red) with a circular and 50 environment initializations.
- [RoboMimic](https://robomimic.github.io/docs/datasets/overview.html): A large-scale, diverse collection of task demonstrations spanning multiple human demonstrators of varying quality, multiple robot manipulation tasks of varying difficulty, and both simulated and real data.
- [Robo Turk](https://roboturk.stanford.edu/): A Crowdsourcing Platform for Robotic Skill Learning through Imitation.
- [RoboNet](https://www.robonet.wiki/): An open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation.

### Revelant Resources

- [LeRobot](https://huggingface.co/lerobot) aims to provide models, datasets, and tools for real-world robotics in PyTorch.

---

:speaker: ***If you would like to acquire datasets of vision-language-action model, please click [here](https://github.com/Evan-wyl/robotlearning/tree/master/fm/data/vla).***

