## Offline to Online RL

[2022] [Leveraging Offline Data in Online Reinforcement Learning](https://arxiv.org/abs/2211.04974)

[2023] [Efficient Online Reinforcement Learning with Offline Data](https://arxiv.org/abs/2302.02948)

[2023] [Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning](https://arxiv.org/abs/2303.05479)

[2023] [Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient](https://arxiv.org/abs/2210.06718)

[2024] [Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning](https://arxiv.org/abs/2309.16984)



### Fine-Tuning

[2020] [AWAC: Accelerating Online Reinforcement Learning with Offline Datasets](https://arxiv.org/abs/2006.09359)

[2021]  [Offline reinforcement learning with implicit q-learning](https://arxiv.org/abs/2110.06169)

[2021] [Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble](https://arxiv.org/abs/2107.00591)

[2022] [Mildly Conservative Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2206.04745)

[2022] [Improving TD3-BC: Relaxed Policy Constraint for Offline Learning and Stable Online Fine-Tuning](https://arxiv.org/abs/2211.11802)

[2022] [Supported Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2202.06239)

[2022] [Fine-tuning offline policies with optimistic action selection](https://openreview.net/pdf?id=ELmiPlCOSw)